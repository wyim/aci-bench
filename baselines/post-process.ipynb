{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process\n",
    "\n",
    "This code creates the evaluation bash script that calls the evaluation for each test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the model logs to save space\n",
    "from glob import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "for path in glob(\"experiments/*\"):\n",
    "        if os.path.isdir(path+\"/runs\"):\n",
    "            shutil.rmtree(path+\"/runs\")\n",
    "        for subpath in glob(path+\"/*\"):\n",
    "             if os.path.isdir(subpath) and \"best_model\" not in subpath:\n",
    "                shutil.rmtree(subpath)\n",
    "\n",
    "for file in glob(\"experiments/**/*.bin\"):\n",
    "    if \"best_model\" not in file:\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine predictions from division-based approaches\n",
    "import json\n",
    "\n",
    "sec_map={\n",
    "    \"subjective\": \"chief complaint :\", \n",
    "    \"objective_exam\":\"physical examination :\", \n",
    "    \"objective_results\": \"results :\",\n",
    "    \"assessment_and_plan\": \"ASSESSMENT AND PLAN\"    \n",
    "}\n",
    "\n",
    "division_types=[\"subjective\",\"objective_exam\", \"objective_results\",\"assessment_and_plan\"]\n",
    "\n",
    "for testset in [\"clinicalnlp_taskB_test1\",\"clinicalnlp_taskC_test2\",\"clef_taskC_test3\"]:#\n",
    "        for model in [\"bart-large-xsum-samsum\",\"BioBART\",\"BART_large\",\"bart-large-xsum-samsum\",\"LED\",\"LED_pubmed\"]:#]:\n",
    "            out_dir=\"experiments/{}_{}_division_combined\".format(model,testset)\n",
    "            if not os.path.isdir(out_dir):\n",
    "                os.mkdir(out_dir)\n",
    "\n",
    "            pred_dir=\"../data/challenge_data_json/{}.json\".format(testset)\n",
    "            \n",
    "            old_pred=json.loads(open(pred_dir).read())['data']\n",
    "            pred=[]\n",
    "\n",
    "            for p in old_pred:\n",
    "                    pred.append({\n",
    "                        \"source\":p[\"src\"],\n",
    "                        \"true\":p[\"tgt\"],\n",
    "                        \"pred\":\"\"\n",
    "                    })\n",
    "\n",
    "            for sec in division_types: \n",
    "                #For bart-based models\n",
    "                source_dir=glob(\"experiments/{}_{}_{}/prediction_{}_{}.json\".format(model,testset,sec,testset,sec))\n",
    "                \n",
    "                #fOR led-BASED MODELS\n",
    "                if not source_dir:\n",
    "                        source_dir=glob(\"experiments/{}_{}_{}/*.json\".format(model,testset,sec))\n",
    "                        source_dir=[file for file in source_dir if \"prediction\" in file]\n",
    "                \n",
    "                if len(source_dir)>1:\n",
    "                    source_dir.sort()\n",
    "                assert len(source_dir)==1\n",
    "                source_dir=source_dir[-1]\n",
    "                section=json.loads(open(source_dir).read())\n",
    "                assert len(section)==len(pred), [len(section),len(pred)]\n",
    "                for i,p in enumerate(pred):\n",
    "                        if \"LED\" in source_dir:\n",
    "                            #as LED models unable to learn the section header.\n",
    "                            pred[i][\"pred\"]+=\"\\n\"+sec_map[sec]+\"\\n\"\n",
    "                        else:\n",
    "                            pred[i][\"pred\"]+=\"\\n\"\n",
    "                        pred[i][\"pred\"]+=section[i][\"pred\"]\n",
    "                \n",
    "            with open(out_dir+\"/prediction.json\",\"w\",encoding=\"utf-8\") as f :\n",
    "                json.dump(pred,f,indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the result\n",
    "\n",
    "1. extract predictions from the baseline/experiment folder, reformat into csv style into the predictions folder.\n",
    "2. generate the bash script for running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "RESOURCE_DIR = '../resource/'\n",
    "\n",
    "CHALLENGE_DATA_DIR = DATA_DIR+'challenge_data_json/' \n",
    "SRCEXP_DATA_dir = DATA_DIR+'src_experiment_data/'\n",
    "\n",
    "testsets = [\"clinicalnlp_taskB_test1\", \"clinicalnlp_taskC_test2\",\"clef_taskC_test3\"]\n",
    "testsets = [\"test1\", \"test2\",\"test3\"]\n",
    "PRED_DIR = \"predictions/\"\n",
    "if not os.path.isdir(PRED_DIR):\n",
    "    os.mkdir(PRED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the reference script for the tabl\n",
    "dataset_map={}\n",
    "\n",
    "#map for the ablation study\n",
    "for testset in testsets:\n",
    "    for testset2 in [ \"aci_asrcorr\", \"aci_asr\", \"virtscribe_asr\", \"virtscribe_humantrans\" ]:\n",
    "        dataset_map[\"{}_{}_\".format(testset,testset2)]=\"{}src_experiment_data/{}_{}.csv\".format(DATA_DIR,testset,testset2)\n",
    "        dataset_map[\"{}_{}.\".format(testset,testset2)]=dataset_map[\"{}_{}_\".format(testset,testset2)]\n",
    "\n",
    "#map for the real study\n",
    "dataset_map={**dataset_map,\n",
    "            **{\n",
    "#            \"valid\":\"%schallenge_data/valid.csv\" %DATA_DIR,\n",
    "            \"test1\":\"%schallenge_data/clinicalnlp_taskB_test1.csv\" %DATA_DIR,\n",
    "            \"taskB\":\"%schallenge_data/clinicalnlp_taskB_test1.csv\" %DATA_DIR,\n",
    "            \"test2\":\"%schallenge_data/clinicalnlp_taskC_test2.csv\" %DATA_DIR,\n",
    "            \"taskC_test2\":\"%schallenge_data/clinicalnlp_taskC_test2.csv\" %DATA_DIR,\n",
    "            \"test3\":\"%schallenge_data/clef_taskC_test3.csv\" %DATA_DIR,\n",
    "            \"taskC_test3\":\"%schallenge_data/clef_taskC_test3.csv\" %DATA_DIR,\n",
    "            }}\n",
    "\n",
    "all_paths=glob(\"experiments/*\")\n",
    "\n",
    "results_to_evaluate=[\"full\",\"division\",\"12_doctor_turns\",\"12_speaker_turns\",\"longest_speaker_turn\",\"longest_doctor_turn\",\n",
    "                    \"spacy_similarity\",\"UMLS_similarity\",\"transcript\"]\n",
    "\n",
    "#print(pred_files)\n",
    "with open(\"evaluation_script.sh\",\"w\") as f:\n",
    "    for path in all_paths:\n",
    "        pred_files=[file for file in glob(path+\"/*.json\") if \"prediction\" in file and \"epoch\" not in file]\n",
    "        if pred_files:\n",
    "            pred_files.sort()\n",
    "            file=pred_files[-1]\n",
    "\n",
    "            #if the result will be included in the table\n",
    "            if any([r in file for r in results_to_evaluate]):\n",
    "                for key in dataset_map:\n",
    "                    if key in file and \"BART\" in file.upper():\n",
    "                            outname=\"predictions/{}.csv\".format(path.split(\"/\")[-1])\n",
    "                            \n",
    "                            #generate prediction file\n",
    "                            pred=json.loads(open(file).read())\n",
    "                            src_df=pd.read_csv(dataset_map[key],encoding=\"utf-8\")\n",
    "                            if len(pred)==len(src_df):\n",
    "                                for ind,p in enumerate(pred):\n",
    "                                    src_df['note'][ind]=p['pred']\n",
    "                                src_df['dataset'][ind]=src_df['dataset'][ind]+\"-{}\".format(ind)\n",
    "                            else:\n",
    "                                print([file,key,\"error\"])\n",
    "                                continue\n",
    "                            \n",
    "                            src_df.to_csv(outname,index=False)\n",
    "                            f.write(\"python evaluation/evaluate_fullnote.py \\\\\\n\")\n",
    "                            f.write(dataset_map[key][3:]+\" \\\\\\n\") # ref\n",
    "                            assert os.path.isfile(dataset_map[key]), dataset_map[key]\n",
    "                            assert  os.path.isfile(outname),outname\n",
    "                            f.write(\"baselines/\"+outname+\" \\\\\\n\") # prediction\n",
    "                            \n",
    "                            meta_file=dataset_map[key].replace(\".csv\",\"_metadata.csv\")\n",
    "                            if os.path.isfile(meta_file):\n",
    "                                f.write(meta_file[3:]+\"\\n\") #write meta-data\n",
    "                            else:\n",
    "                                print(meta_file)\n",
    "                            f.write(\"\\n\\n\\n\")\n",
    "                            break  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the results to tables\n",
    "\n",
    "first, run the evaluation script through\n",
    "\n",
    "```\n",
    "bash ./baselines/evaluation_script.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the files\n",
    "import os\n",
    "from glob import glob\n",
    "name_pairs={\n",
    "\"taskB_organizer\":\"clinicalnlp_taskB_test1\",\n",
    "\"taskC_organizer\":\"clinicalnlp_taskC_test2\",\n",
    "\"clef_organizer\":\"clef_taskC_test3\",\n",
    "\"chatgpt_run2\":\"ChatGPT_\",\n",
    "\"chatgpt_run2\":\"ChatGPT_\",\n",
    "\"davinci2_run2\":\"Text-Davinci-002_\",\n",
    "\"davinci3_run2\":\"Text-Davinci-003_\",\n",
    "\"gpt4_run2\":\"GPT-4_\",\n",
    "\"clinicalnlp_taskB_test1\":\"test1\",\n",
    "\"clinicalnlp_taskC_test2\":\"test2\",\n",
    "\"clef_taskC_test3\":\"test3\",\n",
    "\"_1024\":\"\",\n",
    "\"_gl1024\":\"\",\n",
    "\".csv\":\"\"\n",
    "}\n",
    "\n",
    "# files=glob(\"../baselines/predictions/*.csv\")\n",
    "files=glob(\"../results/*.json\")\n",
    "for file in files:\n",
    "    new_name=file+\"\"\n",
    "    for key in name_pairs:\n",
    "        new_name=new_name.replace(key,name_pairs[key])\n",
    "    os.rename(file,new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "files=glob(\"../results/*.json\")\n",
    "\n",
    "def get_model_result(dataset):\n",
    "    model_map={\n",
    "        \"longest spearker turn\":'../results/longest_speaker_turn_{}.json'.format(dataset),\n",
    "        \"longest doctor turn\":'../results/longest_doctor_turn_{}.json'.format(dataset),\n",
    "        \"12 speaker turns\":'../results/12_speaker_turns_{}.json'.format(dataset),\n",
    "        \"12 doctor turns\": '../results/12_doctor_turns_{}.json'.format(dataset),\n",
    "        \"transcript\":\"../results/transcript_{}.json\".format(dataset),\n",
    "        \"train$_{\\mathrm{UMLS}}$\":\"../results/UMLS_similarity_{}.json\".format(dataset),\n",
    "        \"train$_{\\mathrm{sent}}$\":'../results/spacy_similarity_{}.json'.format(dataset),\n",
    "    }\n",
    "\n",
    "    #read_data\n",
    "    for model in [\"BART_large\",\"bart-large-xsum-samsum\",\"BioBART\",\"LED\",\"LED_pubmed\",\n",
    "                  \"Text-Davinci-002\",\"Text-Davinci-003\",\"ChatGPT\",\"GPT-4\"]:\n",
    "        outfile='../results/{}_{}_full.json'.format(model,dataset)\n",
    "\n",
    "        #open_ai models\n",
    "        if not os.path.isfile(outfile):\n",
    "            outfile='../results/{}{}_.json'.format(dataset,model) \n",
    "        if outfile not in files:\n",
    "            outfile=[file for file in files if \"{}_{}_full\".format(model,dataset) in file]\n",
    "            outfile.sort()\n",
    "            outfile=outfile[-1] if outfile else \"\"\n",
    "\n",
    "        #format the output for latex table\n",
    "        key=model.replace(\"bart-large\",\"BART\").replace(\"_large\",\"\").replace(\"-xsum-samsum\",\"+FT$_{\\mathrm{SAMSum}}$\").replace(\"_pubmed\",\"+FT$_{\\mathrm{PubMed}}$\")\n",
    "        #baseline\n",
    "        model_map[key]=outfile\n",
    "\n",
    "        if model in [\"BART_large\",\"bart-large-xsum-samsum\",\"BioBART\",\"LED\",\"LED_pubmed\"]:\n",
    "            #modelname=f\"{model}_{dataset}_ori_SOAP_combined\" if \"LED\" not in model else f\"{model}_{dataset}_division_combined\"\n",
    "            modelname=f\"{model}_{dataset}_division_combined\"\n",
    "            \n",
    "            outfile=[file for file in files if modelname in file]\n",
    "            model_map[key+\" (Division)\"]=outfile[-1] if outfile else \"\"\n",
    "    return model_map\n",
    "\n",
    "\n",
    "#testsets=[\"clinicalnlp_taskB_test1\",\"clinicalnlp_taskC_test2\",\"clef_taskC_test3\"] #\"train\",\"valid\",\n",
    "testsets=[\"test1\",\"test2\",\"test3\"]\n",
    "# test 1\n",
    "for dataset in testsets:\n",
    "    model_map=get_model_result(dataset)\n",
    "    #map\n",
    "    with open(\"../tables/{}.csv\".format(dataset),\"w\") as f:\n",
    "        for result_type in [\"ALL\", \"division-subjective\",\"division-objective_exam\",\n",
    "                            \"division-objective_results\",\"division-assessment_and_plan\"]:\n",
    "            f.write(\"\\n\\n\"+result_type+\"\\n\")\n",
    "            headers=['Model','ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BERTScore', 'BLEURT','Fact',\"average\"]\n",
    "            f.write(\",\".join(headers)+\"\\n\")\n",
    "            for key in model_map:\n",
    "                if key in [\"BART\",\"LED\"]:\n",
    "                    f.write(\"{}-based\\n\".format(key))\n",
    "                elif key==\"longest spearker turn\":\n",
    "                    f.write(\"Transcript-based\\n\")\n",
    "                elif key==\"train$_{\\mathrm{UMLS}}$\":\n",
    "                    f.write(\"Retrieval-based\\n\")\n",
    "                elif key==\"Text-Davinci-002\":\n",
    "                    f.write(\"OpenAI (wo FT)\\n\")\n",
    "                row=[key]\n",
    "                if os.path.isfile(model_map[key]):\n",
    "                    result=json.loads(open(model_map[key]).read())[result_type]\n",
    "                    for metric in ['rouge1', 'rouge2', 'rougeLsum', 'bertscore-f1', 'bleurt','umls']:\n",
    "                        row.append(float(result[metric]))\n",
    "                    row.append(((sum(row[1:4])/3+sum(row[4:7])))/4)\n",
    "                    row=[row[0]]+[\"{:.2f}\".format(round(r*100,2)) for r in row[1:]]\n",
    "                else:\n",
    "                    row=row+[\"NA\"]*7\n",
    "                f.write(\",\".join(row)+\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output to latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full note\n",
    "Full_note_begin=\"\"\"\n",
    "\\\\begin{table}[]\n",
    "\\\\centering\n",
    "\\\\begin{tabular}{lcccc}\n",
    "\\\\hline\n",
    "\\\\textbf{Model}                           & \\\\textbf{ROUGE-1} & \\\\textbf{ROUGE-2} & \\\\textbf{ROUGE-L} & \\\\textbf{Fact} \\\\\\\\ \\\\hline\n",
    "\"\"\"\n",
    "\n",
    "Full_note_end=\"\"\"\n",
    "\\\\hline\n",
    "\\\\end{tabular}\n",
    "\\\\caption{Results of the summarization models fine-tuned (FT) and evaluated on the ACI demo corpus, test set 1.}\n",
    "\\\\label{tab:test1}\n",
    "\\\\end{table}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tex(text,filename):\n",
    "    with open(filename,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    return\n",
    "\n",
    "result_type=\"ALL\"\n",
    "metrics=['rouge1', 'rouge2', 'rougeLsum', 'umls']\n",
    "for dataset in testsets:\n",
    "\n",
    "        max_values={}\n",
    "\n",
    "        filename=f\"../tables/{dataset}.tex\"\n",
    "        model_map=get_model_result(dataset)\n",
    "\n",
    "        text=Full_note_begin\n",
    "        \n",
    "        headers=['Model','ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Fact']\n",
    "\n",
    "        for key in model_map:\n",
    "                    if key in [\"BART\",\"LED\"]:\n",
    "                        text+=\"\\\\cline{1-1}\\n\\\\textbf{\"+key+\"-based} & & & &\\\\\\\\\"\n",
    "                    elif key==\"longest spearker turn\":\n",
    "                        text+=\"\\\\textbf{Transcript-copy-and-paste} & & & &\\\\\\\\\"\n",
    "                    elif key==\"train$_{\\mathrm{UMLS}}$\":\n",
    "                        text+=\"\\\\cline{1-1}\\n\\\\textbf{Retrieval-based} & & & &\\\\\\\\\"\n",
    "                    elif key==\"Text-Davinci-002\":\n",
    "                        text+=\"\\\\cline{1-1}\\n\\\\textbf{OpenAI (wo FT)} & & & &\\\\\\\\\"\n",
    "\n",
    "                    row=[key]\n",
    "                    if os.path.isfile(model_map[key]):\n",
    "                        result=json.loads(open(model_map[key]).read())[result_type]\n",
    "                        for metric in metrics:\n",
    "                            row.append(float(result[metric]))\n",
    "                            max_values[metric]=max(max_values.get(metric,row[-1]),row[-1])\n",
    "                        row=[row[0]]+[\"{:.2f}\".format(round(r*100,2)) for r in row[1:]]\n",
    "                    else:\n",
    "                        row=row+[\"NA\"]*4\n",
    "                    text+=\"&\".join(row)+\"\\\\\\\\ \\n\"\n",
    "\n",
    "        text+=Full_note_end\n",
    "\n",
    "        text=text.replace(\"\\n\\\\cline{1-1}\",\"\\\\cline{1-1}\")\n",
    "\n",
    "        # highlight the max values\n",
    "        for metric in metrics:\n",
    "            number=\"{:.2f}\".format(round(max_values[metric]*100,2))\n",
    "            if text.count(number)==1:\n",
    "                text=text.replace(number,\"\\\\textbf{\"+number+\"}\")\n",
    "            else:\n",
    "                print(\"error\")\n",
    "        \n",
    "        text=text.replace(\"test set 1\",\"test set {}\".format(dataset[-1]))\n",
    "        text=text.replace(\"test1\",dataset)\n",
    "        write_tex(text,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devision-based\n",
    "division_begin=\"\"\"\n",
    "\\\\begin{table}[]\n",
    "\\\\centering\n",
    "\\\\begin{tabular}{lccccccc}\n",
    "\\\\hline\n",
    "\\\\textbf{} & \\\\multicolumn{7}{c}{\\\\textbf{Evaluation score on the assessment\\\\_and\\\\_plan division}} \\\\\\\\ \\\\cline{2-8} \n",
    "\\\\textbf{Model} & \\\\textbf{ROUGE-1} & \\\\textbf{ROUGE-2} & \\\\textbf{ROUGE-L} & \\\\textbf{BERTScore} & \\\\textbf{BLEURT} & \\\\textbf{Fact} & \\\\textbf{Average} \\\\\\\\ \\\\hline\n",
    "\"\"\"\n",
    "\n",
    "division_end=\"\"\"\n",
    "\\\\hline\n",
    "\\\\end{tabular}\n",
    "\\\\caption{Results of the summarization models on the assessment\\\\_and\\\\_plan division, test set 1.}\n",
    "\\\\label{tab:test1_assessment_and_plan}\n",
    "\\\\end{table}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=['rouge1', 'rouge2', 'rougeLsum', 'bertscore-f1', 'bleurt','umls']\n",
    "for result_type in [\"subjective\",\"objective_exam\",\"objective_results\",\"assessment_and_plan\"]:\n",
    "    for dataset in testsets:\n",
    "\n",
    "        max_values={}\n",
    "\n",
    "        filename=f\"../tables/{dataset}_{result_type}.tex\"\n",
    "        model_map=get_model_result(dataset)\n",
    "\n",
    "        text=division_begin\n",
    "        \n",
    "        headers=['Model','ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Fact']\n",
    "\n",
    "        keys=list(model_map.keys())[5:]\n",
    "        for key in keys:\n",
    "                    if key in [\"BART\",\"LED\"]:\n",
    "                        text+=\"\\\\cline{1-1}\\n\\\\textbf{\"+key+\"-based }&  &  &  &  &  &  &\\\\\\\\ \\n\"\n",
    "                    elif key==\"longest spearker turn\":\n",
    "                        text+=\"\\\\textbf{Transcript-copy-and-paste} &  &  &  &  &  &  &\\\\\\\\ \\n\"\n",
    "                    elif key==\"train$_{\\mathrm{UMLS}}$\":\n",
    "                        text+=\"\\\\textbf{Retrieval-based} &  &  &  &  &  &  &\\\\\\\\ \\n\"\n",
    "                    elif key==\"Text-Davinci-002\":\n",
    "                        text+=\"\\\\cline{1-1}\\n\\\\textbf{OpenAI (wo FT)} &  &  &  &  &  &  &\\\\\\\\ \\n\"\n",
    "\n",
    "                    row=[key]\n",
    "                    if os.path.isfile(model_map[key]):\n",
    "                        result=json.loads(open(model_map[key]).read())[\"division-\"+result_type]\n",
    "                        for metric in metrics:\n",
    "                            row.append(float(result[metric]))\n",
    "                            max_values[metric]=max(max_values.get(metric,row[-1]),row[-1])\n",
    "                        row.append(((sum(row[1:4])/3+sum(row[4:7])))/4)\n",
    "                        max_values[\"Average\"]=max(max_values.get(\"Average\",row[-1]),row[-1])\n",
    "                        row=[row[0]]+[\"{:.2f}\".format(round(r*100,2)) for r in row[1:]]\n",
    "                    else:\n",
    "                        row=row+[\"NA\"]*7\n",
    "                    text+=\"&\".join(row)+\"\\\\\\\\ \\n\"\n",
    "\n",
    "        text+=division_end\n",
    "\n",
    "        text=text.replace(\"\\n\\\\cline{1-1}\",\"\\\\cline{1-1}\")\n",
    "\n",
    "        # highlight the max values\n",
    "        for metric in metrics+[\"Average\"]:\n",
    "            number=\"{:.2f}\".format(round(max_values[metric]*100,2))\n",
    "            if text.count(number)==1:\n",
    "                text=text.replace(number,\"\\\\textbf{\"+number+\"}\")\n",
    "            else:\n",
    "                print([metric,dataset,result_type])\n",
    "        \n",
    "        text=text.replace(\"test set 1\",\"test set {}\".format(dataset[-1]))\n",
    "        text=text.replace(\"test1\",dataset)\n",
    "        text=text.replace(\"assessment_and_plan\",result_type)\n",
    "        text=text.replace(\"assessment\\\\_and\\\\_plan\",result_type.replace(\"_\",\"\\\\_\"))\n",
    "        write_tex(text,filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read prediction example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=\"/home/velvinfu/code/aci-demo-benchmark-private-main/baselines/experiments/2_bart-large-xsum-samsum_clinicalnlp_taskC_test2_full/prediction_clinicalnlp_taskC_test2_full.json\"\n",
    "file2=\"/home/velvinfu/code/clef2023-internal/predictions/bart-large-xsum-samsum_test2_full_ori.json\"\n",
    "file3=\"/home/velvinfu/code/aci-demo-benchmark-private-main/baselines/experiments/bart-large-xsum-samsum_clinicalnlp_taskC_test2_full/prediction_clinicalnlp_taskC_test2_full.json\"\n",
    "\n",
    "import json\n",
    "result1=json.loads(open(file1).read())[0][\"pred\"]\n",
    "result2=json.loads(open(file2).read())[0][\"pred\"]\n",
    "result3=json.loads(open(file3).read())[0][\"pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1==result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_acidemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
