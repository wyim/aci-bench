{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# baseline_transcript_retrieval.ipynb\n",
        "\n",
        "### This file contains the transcript-copy and IR retreival baselines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "RESOURCE_DIR = './../resources'\n",
        "DATA_DIR = './../data'\n",
        "testsets=[\"valid\",\"clinicalnlp_taskB_test1\",\"clinicalnlp_taskC_test2\",\"clef_taskC_test3\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# retrieval-based: spacy sentence similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#spacy similarity\n",
        "import spacy\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity \n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "train_file=\"{}/challenge_data_json/train.json\".format(DATA_DIR)\n",
        "data=json.loads(open(train_file).read())[\"data\"]\n",
        "\n",
        "source=[]\n",
        "for dic in data:\n",
        "        source.append(nlp(dic[\"src\"]))\n",
        "for testset in testsets:    \n",
        "        valid_file=\"{}/challenge_data_json/{}.json\".format(DATA_DIR,testset)\n",
        "        tgt=json.loads(open(valid_file).read())[\"data\"]\n",
        "        \n",
        "        out=[]\n",
        "        for dic in tgt:\n",
        "                embedding = nlp(dic['src'])\n",
        "                similarity = [embedding.similarity(d) for d in source]\n",
        "                index=np.argmax(similarity)\n",
        "                out.append({\"source\":dic[\"src\"],\"true\":dic[\"tgt\"],\"pred\":data[index][\"tgt\"]})\n",
        "        dirt=f\"experiments/spacy_similarity_{testset}/\"\n",
        "        if not os.path.isdir(dirt):\n",
        "                os.mkdir(dirt)\n",
        "        with open(dirt+\"prediction.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "                json.dump(out,f,indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# retrieval-based: UMLS similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#UMLS simialrity\n",
        "import spacy\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "quickumls_fp = \"{}/des/\".format(RESOURCE_DIR)\n",
        "#the window size for transitioning\n",
        "WINDOW_SIZE=5\n",
        "COUNT_THRESHOLD=50\n",
        "ENCODING=\"utf-8\"\n",
        "from semantics import SEMANTICS\n",
        "def get_matches(text,use_umls=True):\n",
        "    concepts={}\n",
        "    cui_list=[]\n",
        "    if use_umls:\n",
        "        matches=matcher.match(text, ignore_syntax=True)\n",
        "        for match in matches:\n",
        "            for m in match:\n",
        "                if m['cui'] not in concepts.get(m['term'],[]):\n",
        "                    concepts[m['term']]=concepts.get(m['term'],[])+[m['cui']]\n",
        "                    cui_list.append(m['cui'])\n",
        "    else:\n",
        "        doc = nlp(text)\n",
        "        #linker = nlp.get_pipe(\"scispacy_linker\")\n",
        "        for ent in doc.ents:\n",
        "            key=(ent.text.lower(),ent.label_)\n",
        "            if ent.text not in concepts.get(key,[]):\n",
        "                    concepts[key]=concepts.get(key,[])+[ent.text]\n",
        "                    cui_list.append(ent.text)\n",
        "    return concepts,cui_list\n",
        "\n",
        "from quickumls import QuickUMLS\n",
        "matcher = QuickUMLS(quickumls_fp,window=WINDOW_SIZE,threshold=1,accepted_semtypes=SEMANTICS)\n",
        "\n",
        "train_file=\"{}/challenge_data_json/train.json\".format(DATA_DIR)\n",
        "data=json.loads(open(train_file).read())[\"data\"]\n",
        "\n",
        "source=[]\n",
        "for dic in data:\n",
        "        source.append(dic['src'])\n",
        "def umls_score_individual(reference,prediction,use_umls=True):\n",
        "    true_concept,true_cuis=get_matches(reference,use_umls)\n",
        "    pred_concept,pred_cuis=get_matches(prediction,use_umls)\n",
        "    try:\n",
        "        num_t=0\n",
        "        for key in true_concept:\n",
        "            for cui in true_concept[key]:\n",
        "                if cui in pred_cuis:\n",
        "                    num_t+=1\n",
        "                    break\n",
        "        \n",
        "        precision=num_t*1.0/len(pred_concept.keys())\n",
        "        recall=num_t*1.0/len(true_concept.keys())\n",
        "        F1=2*(precision*recall)/(precision+recall)\n",
        "        return F1\n",
        "    except:\n",
        "        return 0\n",
        "for testset in testsets:    \n",
        "    valid_file=\"{}/challenge_data_json/{}.json\".format(DATA_DIR,testset)\n",
        "    tgt=json.loads(open(valid_file).read())[\"data\"]\n",
        "        \n",
        "    out=[]\n",
        "    for dic in tqdm(tgt):\n",
        "            similarity = [umls_score_individual(reference,dic['src'],use_umls=True) for reference in source]\n",
        "            #print(similarity)\n",
        "            index=np.argmax(similarity)\n",
        "            out.append({\"source\":dic[\"src\"],\"true\":dic[\"tgt\"],\"pred\":data[index][\"tgt\"]})\n",
        "    dirt=f\"experiments/UMLS_similarity_{testset}/\"\n",
        "    if not os.path.isdir(dirt):\n",
        "            os.mkdir(dirt)\n",
        "    with open(dirt+\"prediction.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "            json.dump(out,f,indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# transcript as baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#transcript as baseline\n",
        "import json\n",
        "import os\n",
        "for testset in testsets:    \n",
        "        valid_file=\"{}/challenge_data_json/{}.json\".format(DATA_DIR,testset)\n",
        "        tgt=json.loads(open(valid_file).read())[\"data\"]\n",
        "        \n",
        "        out=[]\n",
        "        for dic in tgt:\n",
        "                out.append({\"source\":dic[\"src\"],\"true\":dic[\"tgt\"],\"pred\":dic[\"src\"]})\n",
        "        dirt=f\"experiments/transcript_{testset}/\"\n",
        "        if not os.path.isdir(dirt):\n",
        "                os.mkdir(dirt)\n",
        "        with open(dirt+\"prediction.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "                json.dump(out,f,indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# part of transcript as baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#first 2 doctor turns and the last 10 turns for doctors\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "for testset in testsets:    \n",
        "        valid_file=\"{}/challenge_data_json/{}.json\".format(DATA_DIR,testset)\n",
        "        tgt=json.loads(open(valid_file).read())[\"data\"]\n",
        "        \n",
        "        out=[]\n",
        "        for dic in tgt:\n",
        "                doctors=dic[\"src\"].replace(\"[patient_guest]\",\"[patient]\").split(\"[doctor]\")\n",
        "                \n",
        "                doctors=[d.split(\"[patient]\")[0] for d in doctors if d]\n",
        "                if len(doctors)>12:\n",
        "                        to_include=doctors[:2]+doctors[len(doctors)-10:]\n",
        "                else:\n",
        "                        to_include=doctors\n",
        "                assert len(to_include)<=12\n",
        "                out.append({\"source\":dic[\"src\"],\"true\":dic[\"tgt\"],\"pred\":\"\\n\".join(to_include)})\n",
        "        dirt=f\"experiments/12_doctor_turns_{testset}/\"\n",
        "        if not os.path.isdir(dirt):\n",
        "                os.mkdir(dirt)\n",
        "        with open(dirt+\"prediction.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "                json.dump(out,f,indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#first 2 speaker turns and the last 10 speaker turns for doctors\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "for testset in testsets:    \n",
        "        valid_file=\"{}/challenge_data_json/{}.json\".format(DATA_DIR,testset)\n",
        "        tgt=json.loads(open(valid_file).read())[\"data\"]\n",
        "        \n",
        "        out=[]\n",
        "        for dic in tgt:\n",
        "                doctors=dic[\"src\"].replace(\"[patient_guest]\",\"[patient]\")\n",
        "                #assert doctors.count(\"[\")==doctors.count(\"[patient]\")+doctors.count(\"[doctor]\"),[doctors.count(\"[\"),doctors.count(\"[patient]\")+doctors.count(\"[doctor]\")]\n",
        "                doctors=doctors.split(\"[\")\n",
        "                doctors=[d for d in doctors if d]\n",
        "                \n",
        "                #doctors=[d.split(\"[patient]\")[0] for d in doctors if d]\n",
        "                if len(doctors)>12:\n",
        "                        to_include=doctors[:2]+doctors[len(doctors)-10:]\n",
        "                else:\n",
        "                        to_include=doctors\n",
        "                assert len(to_include)<=12\n",
        "                out.append({\"source\":dic[\"src\"],\"true\":dic[\"tgt\"],\"pred\":\"[\".join(to_include)})\n",
        "        dirt=f\"experiments/12_speaker_turns_{testset}/\"\n",
        "        if not os.path.isdir(dirt):\n",
        "                os.mkdir(dirt)\n",
        "        with open(dirt+\"prediction.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "                json.dump(out,f,indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#longest doctor turn\n",
        "#spacy similarity\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "for testset in testsets:    \n",
        "        valid_file=\"{}/challenge_data_json/{}.json\".format(DATA_DIR,testset)\n",
        "        tgt=json.loads(open(valid_file).read())[\"data\"]\n",
        "        \n",
        "        out=[]\n",
        "        for dic in tgt:\n",
        "                doctors=dic[\"src\"].replace(\"[patient_guest]\",\"[patient]\").split(\"[doctor]\")\n",
        "                doctors=[d.split(\"[patient]\")[0] for d in doctors if d]\n",
        "                current_length=0\n",
        "                for d in doctors:\n",
        "                    if len(d.split())>current_length:\n",
        "                           to_include=d\n",
        "                           current_length=len(d.split())\n",
        "                out.append({\"source\":dic[\"src\"],\"true\":dic[\"tgt\"],\"pred\":to_include})\n",
        "        dirt=f\"experiments/longest_doctor_turn_{testset}/\"\n",
        "        if not os.path.isdir(dirt):\n",
        "                os.mkdir(dirt)\n",
        "        with open(dirt+\"prediction.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "                json.dump(out,f,indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#longest doctor turn\n",
        "import json\n",
        "import os\n",
        "\n",
        "for testset in testsets:    \n",
        "        valid_file=\"{}/challenge_data_json/{}.json\".format(DATA_DIR,testset)\n",
        "        tgt=json.loads(open(valid_file).read())[\"data\"]\n",
        "        \n",
        "        out=[]\n",
        "        for dic in tgt:\n",
        "                doctors=dic[\"src\"].replace(\"[patient_guest]\",\"[patient]\").replace(\"[patient]\",\"[doctor]\").split(\"[doctor]\")\n",
        "                current_length=0\n",
        "                for d in doctors:\n",
        "                    if len(d.split())>current_length:\n",
        "                           to_include=d\n",
        "                           current_length=len(d.split())\n",
        "                out.append({\"source\":dic[\"src\"],\"true\":dic[\"tgt\"],\"pred\":to_include})\n",
        "        dirt=f\"experiments/longest_speaker_turn_{testset}/\"\n",
        "        if not os.path.isdir(dirt):\n",
        "                os.mkdir(dirt)\n",
        "        with open(dirt+\"prediction.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "                json.dump(out,f,indent=4)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9bd55647ee4f047411bc4f583ac9d2377755c15697ebb42ef2168cc78e26f0c1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
