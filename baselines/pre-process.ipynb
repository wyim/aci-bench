{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocess data\n",
        "\n",
        "## This file processes the original CSV format into json formats and also tags the data with UMLS concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = '../data/'\n",
        "RESOURCE_DIR = '../resources/des/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONVERT CSV format to JSON format\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "\n",
        "#dataset,id,doctor_name,doctor_gender,patient_gender,patient_age,patient_firstname,patient_familyname,cc,2nd_complaints,note,dialogue\n",
        "#\n",
        "\n",
        "def check_dir(dir_path):\n",
        "    if not os.path.isdir(dir_path):\n",
        "        os.mkdir(dir_path)\n",
        "    return\n",
        "\n",
        "def output_json_versions( all_files ) :\n",
        "\n",
        "    all_files=[file for file in all_files if \"_metadata\" not in file]\n",
        "    all_files.sort()\n",
        "    for file in all_files:\n",
        "            df=pd.read_csv(file,encoding=\"utf-8\")\n",
        "\n",
        "            out=[]\n",
        "            for ind in df.index:\n",
        "                id=df['encounter_id'][ind] if 'encounter_id' in df else str(ind)\n",
        "                out.append({\n",
        "                            \"src\":df['dialogue'][ind],\n",
        "                            \"tgt\":df['note'][ind],\n",
        "                            \"file\":id+\"-\"+str(df['dataset'][ind]),\n",
        "                        })\n",
        "            out_name=file.replace(\"src_experiment_data\",\"src_experiment_data_json\").replace(\"challenge_data\",\"challenge_data_json\").replace(\".csv\",\".json\")\n",
        "            with open(out_name,\"w\",encoding=\"utf-8\") as f:\n",
        "                json.dump({\"data\":out},f,indent=4)\n",
        "        \n",
        "\n",
        "# pre-process baseline experiment data\n",
        "all_files=glob( \"%s/challenge_data/*.csv\" %DATA_DIR )\n",
        "check_dir(\"%s/challenge_data_json\" %DATA_DIR)\n",
        "output_json_versions( all_files )\n",
        "\n",
        "# pre-process ablation experiment data\n",
        "all_files=glob( \"%s/src_experiment_data/*.csv\" %DATA_DIR )\n",
        "check_dir(\"%s/src_experiment_data_json\" %DATA_DIR)\n",
        "output_json_versions( all_files )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# label json data with UMLS concepts\n",
        "\n",
        "This is just for the ablation study, where we label the UMLS concepts in the dialogue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#define the fact-based extractor\n",
        "quickumls_fp = \"%s\" %RESOURCE_DIR\n",
        "\n",
        "#the window size for transitioning\n",
        "WINDOW_SIZE=5\n",
        "COUNT_THRESHOLD=50\n",
        "ENCODING=\"utf-8\"\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
        "\n",
        "\n",
        "from semantics import SEMANTICS\n",
        "\n",
        "from quickumls import QuickUMLS\n",
        "matcher = QuickUMLS(quickumls_fp,window=WINDOW_SIZE,threshold=1,accepted_semtypes=SEMANTICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_umls(text):\n",
        "    matches=matcher.match(text, ignore_syntax=True)\n",
        "    UMLS_set=[]\n",
        "    for match in matches:\n",
        "        #print([m['semtypes'] for m in match])\n",
        "        UMLS_set.append([match[0]['start'],match[0]['end'],\", \".join(set([w for m in match for w in m['semtypes']]))])\n",
        "        #print(match)\n",
        "    UMLS_set.sort(key = lambda x: [x[0],x[1]])\n",
        "\n",
        "    result=text[:UMLS_set[0][0]]\n",
        "    for i,(s,e,type) in enumerate(UMLS_set):\n",
        "        result+=\"[{}]\".format(text[s:e])#\"[{}]({})\".format(text[s:e],type)\n",
        "        if i<len(UMLS_set)-1:\n",
        "            result+=text[e:UMLS_set[i+1][0]]\n",
        "        else:\n",
        "            result+=text[e:]\n",
        "    return result\n",
        "\n",
        "import shutil\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "for dataset in [\"train\",\"valid\",\"clinicalnlp_taskB_test1\",\"clinicalnlp_taskC_test2\",\"clef_taskC_test3\"]:\n",
        "    file= \"%s/challenge_data_json/\" %DATA_DIR + \"{}.json\".format(dataset)\n",
        "    dic=json.loads(open(file,encoding=\"utf-8\").read())[\"data\"]\n",
        "\n",
        "    for i in tqdm(range(len(dic))):\n",
        "        dic[i][\"src\"]=add_umls(dic[i][\"src\"])\n",
        "    with open(file.replace(\".json\",\"_UMLS.json\"),\"w\") as f:\n",
        "        json.dump({\"data\":dic},f,indent=4)\n",
        "    with open(file.replace(\".json\",\"_UMLS_full.json\"),\"w\") as f:\n",
        "        json.dump({\"data\":dic},f,indent=4)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6a9312a832544d35a8cadcbc7e95a4a40f483fb068a2b360c4516339384872e3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
